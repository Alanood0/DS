{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alanood0/DS/blob/main/Activity3_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = blue size = 6> **Activity #3 (5 marks)**"
      ],
      "metadata": {
        "id": "9Hr2lLFuZjja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/https://github.com/Alanood0/DS)"
      ],
      "metadata": {
        "id": "Uu8AQJxWdPFH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yts1Q_C9dH42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font color = green size = 5> **Activity: Air Quality Analysis in NYC**"
      ],
      "metadata": {
        "id": "bEtXiTwI-yIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective:\n",
        "- **Objective:**  \n",
        "The goal of this assignment is to analyze air quality data collected from low-cost sensors mounted on moving vehicles in New York City. Using the concepts learned in the chapters on Sampling and Empirical Distributions , Testing Hypotheses , and Estimation , you will perform statistical analysis to estimate pollution levels, test hypotheses about differences in pollution across neighborhoods, and construct confidence intervals for key parameters.\n",
        "\n",
        "You are provided with a `Datascience` `Table` named `joined_table`, which contains air quality readings (`pm10`) and their corresponding administrative divisions (neighborhoods) in NYC. The table has been preprocessed and joined with neighborhood boundaries using geospatial operations. Your task is to perform all subsequent tasks using the `Datascience` Table abstraction as taught in class."
      ],
      "metadata": {
        "id": "6i3-zemYyZAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "given a csv file containing longitude, latitude , and pm10  columns [Air Quality data](https://raw.githubusercontent.com/IsamAljawarneh/datasets/master/data/NYC_PM.csv) representing readings of low cost air quality sensor mounted on moving vehicles, in addition to a geojson file containing polygons representing administrative divisions of NYC city known as neighbourhoods [nyc_polygon.geojson](https://raw.githubusercontent.com/IsamAljawarneh/datasets/master/data/nyc_polygon.geojson).\n",
        "# Dataset Description\n",
        "- Air Quality Sensor Readings (NYC_PM.csv) :\n",
        "Attributes: SensorID, time, temperature, humidity, pm25,\n",
        "Focus attributes: temperature, humidity, pm1,pm25,pm10,\n",
        "- City Polygons (nyc_polygon.geojson) :\n",
        "Contains polygons representing neighborhoods or boroughs in NYC.\n",
        "Used for spatially joining geographic information with air quality data."
      ],
      "metadata": {
        "id": "MnPCkUN9-lzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **part - A** preprocessing [0 marks]\n",
        "\n",
        "do all tasks and the subtasks!"
      ],
      "metadata": {
        "id": "_kuVRsJkAh9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Onboarding Code Provided\n",
        "- The following code will be provided in an onboarding Jupyter Notebook to help students get started:\n",
        "\n"
      ],
      "metadata": {
        "id": "cXRH1R-SzPAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/drive')'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "R4ArO-FctZIu",
        "outputId": "9a81603c-6c0f-4f9c-aef6-5cbeb860de2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/drive')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Woo7S3Fufvqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import necessary libraries"
      ],
      "metadata": {
        "id": "9o9eM8IICM-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "DIyY1q36ta3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datascience import *\n",
        "%matplotlib inline\n",
        "#path_data = '../../../assets/data/'\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8ipkPRynmWGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Read the CSV file containing PM sensor readings\n",
        " & Read the GeoJSON file containing neighborhood boundaries into a GeoDataFrame"
      ],
      "metadata": {
        "id": "11VvbD-a6ur8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Read the CSV file containing PM10 sensor readings\n",
        "pm10_data = pd.read_csv('https://raw.githubusercontent.com/IsamAljawarneh/datasets/master/data/NYC_PM.csv',index_col=False)\n",
        "\n",
        "# Step 2: Read the GeoJSON file containing neighborhood boundaries into a GeoDataFrame\n",
        "nyc_neighborhoods = gpd.read_file('https://raw.githubusercontent.com/IsamAljawarneh/datasets/master/data/nyc_polygon.geojson')\n"
      ],
      "metadata": {
        "id": "CYk7KfJqueQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pm10_data.dtypes"
      ],
      "metadata": {
        "id": "VAMLIDAKu289"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. convert the csv into a geodataframe and join it (sjoin) with the geojson, assign a coordinate reference system (CRS) the csv geodataframe which is identical to that of the geojson file, then perform the join, the result is a geodataframe, convert it to dataframe, and select pm10, neighborhood columns in a new dataframe"
      ],
      "metadata": {
        "id": "tKKuGLIl682s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pm10_gdf = gpd.GeoDataFrame(pm10_data, geometry=gpd.points_from_xy(pm10_data.longitude, pm10_data.latitude))\n",
        "merged_data = gpd.sjoin(pm10_gdf, nyc_neighborhoods, how='inner', predicate='within')"
      ],
      "metadata": {
        "id": "LDdZ3JM9vABk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "256026e9-7089-4322-8577-9a4f4d57642b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-d43866676e2a>:2: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
            "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
            "\n",
            "Left CRS: None\n",
            "Right CRS: EPSG:4326\n",
            "\n",
            "  merged_data = gpd.sjoin(pm10_gdf, nyc_neighborhoods, how='inner', predicate='within')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#merged_data.dtypes"
      ],
      "metadata": {
        "id": "4YTL56jpvPDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pollution_data = merged_data[['pm10','neighborhood']]"
      ],
      "metadata": {
        "id": "1XPU0kp4hwnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pollution_data.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3gx_my1iAoi",
        "outputId": "0ceb7686-5f4b-47af-e93a-646e5dd0a2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118495"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#merged_data.rename(columns={'neighborhood': 'neighborhood1'}, inplace=True)"
      ],
      "metadata": {
        "id": "P4M9bOYqy09e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(pollution_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "a8C-waCXk8JE",
        "outputId": "45a8d50a-97d0-42a0-c7d8-0cf20c0e2be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.frame.DataFrame</b><br/>def __init__(data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py</a>Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n",
              "\n",
              "Data structure also contains labeled axes (rows and columns).\n",
              "Arithmetic operations align on both row and column labels. Can be\n",
              "thought of as a dict-like container for Series objects. The primary\n",
              "pandas data structure.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n",
              "    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n",
              "    data is a dict, column order follows insertion-order. If a dict contains Series\n",
              "    which have an index defined, it is aligned by its index. This alignment also\n",
              "    occurs if data is a Series or a DataFrame itself. Alignment is done on\n",
              "    Series/DataFrame inputs.\n",
              "\n",
              "    If data is a list of dicts, column order follows insertion-order.\n",
              "\n",
              "index : Index or array-like\n",
              "    Index to use for resulting frame. Will default to RangeIndex if\n",
              "    no indexing information part of input data and no index provided.\n",
              "columns : Index or array-like\n",
              "    Column labels to use for resulting frame when data does not have them,\n",
              "    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n",
              "    will perform column selection instead.\n",
              "dtype : dtype, default None\n",
              "    Data type to force. Only a single dtype is allowed. If None, infer.\n",
              "copy : bool or None, default None\n",
              "    Copy data from inputs.\n",
              "    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n",
              "    or 2d ndarray input, the default of None behaves like ``copy=False``.\n",
              "    If data is a dict containing one or more Series (possibly of different dtypes),\n",
              "    ``copy=False`` will ensure that these inputs are not copied.\n",
              "\n",
              "    .. versionchanged:: 1.3.0\n",
              "\n",
              "See Also\n",
              "--------\n",
              "DataFrame.from_records : Constructor from tuples, also record arrays.\n",
              "DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n",
              "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
              "read_table : Read general delimited file into DataFrame.\n",
              "read_clipboard : Read text from clipboard into DataFrame.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Please reference the :ref:`User Guide &lt;basics.dataframe&gt;` for more information.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing DataFrame from a dictionary.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [1, 2], &#x27;col2&#x27;: [3, 4]}\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d)\n",
              "&gt;&gt;&gt; df\n",
              "   col1  col2\n",
              "0     1     3\n",
              "1     2     4\n",
              "\n",
              "Notice that the inferred dtype is int64.\n",
              "\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int64\n",
              "col2    int64\n",
              "dtype: object\n",
              "\n",
              "To enforce a single dtype:\n",
              "\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d, dtype=np.int8)\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int8\n",
              "col2    int8\n",
              "dtype: object\n",
              "\n",
              "Constructing DataFrame from a dictionary including Series:\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [0, 1, 2, 3], &#x27;col2&#x27;: pd.Series([2, 3], index=[2, 3])}\n",
              "&gt;&gt;&gt; pd.DataFrame(data=d, index=[0, 1, 2, 3])\n",
              "   col1  col2\n",
              "0     0   NaN\n",
              "1     1   NaN\n",
              "2     2   2.0\n",
              "3     3   3.0\n",
              "\n",
              "Constructing DataFrame from numpy ndarray:\n",
              "\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n",
              "...                    columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n",
              "&gt;&gt;&gt; df2\n",
              "   a  b  c\n",
              "0  1  2  3\n",
              "1  4  5  6\n",
              "2  7  8  9\n",
              "\n",
              "Constructing DataFrame from a numpy ndarray that has labeled columns:\n",
              "\n",
              "&gt;&gt;&gt; data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n",
              "...                 dtype=[(&quot;a&quot;, &quot;i4&quot;), (&quot;b&quot;, &quot;i4&quot;), (&quot;c&quot;, &quot;i4&quot;)])\n",
              "&gt;&gt;&gt; df3 = pd.DataFrame(data, columns=[&#x27;c&#x27;, &#x27;a&#x27;])\n",
              "...\n",
              "&gt;&gt;&gt; df3\n",
              "   c  a\n",
              "0  3  1\n",
              "1  6  4\n",
              "2  9  7\n",
              "\n",
              "Constructing DataFrame from dataclass:\n",
              "\n",
              "&gt;&gt;&gt; from dataclasses import make_dataclass\n",
              "&gt;&gt;&gt; Point = make_dataclass(&quot;Point&quot;, [(&quot;x&quot;, int), (&quot;y&quot;, int)])\n",
              "&gt;&gt;&gt; pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n",
              "   x  y\n",
              "0  0  0\n",
              "1  0  3\n",
              "2  2  3\n",
              "\n",
              "Constructing DataFrame from Series/DataFrame:\n",
              "\n",
              "&gt;&gt;&gt; ser = pd.Series([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=ser, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df\n",
              "   0\n",
              "a  1\n",
              "c  3\n",
              "\n",
              "&gt;&gt;&gt; df1 = pd.DataFrame([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], columns=[&quot;x&quot;])\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(data=df1, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df2\n",
              "   x\n",
              "a  1\n",
              "c  3</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 509);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. you need to convert</h1></section> from dataframe to Datascience Table. Use the following format: ```Table.from_df(df, keep_index=False)``` read more here\n",
        "[create DS Table from DF](https://www.data8.org/datascience/_autosummary/datascience.tables.Table.from_df.html)\n",
        "\n",
        "**N.B.** <font color='red'>NOW, perform all tasks using the table abstraction as we have learned in the class!</font>"
      ],
      "metadata": {
        "id": "ajH-jZyp8neJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the following is the opposite:\n",
        "\n",
        "[Table.to_df](https://www.data8.org/datascience/_autosummary/datascience.tables.Table.to_df.html)"
      ],
      "metadata": {
        "id": "ansU7tkQ9cj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "what is the maximum pm10 value"
      ],
      "metadata": {
        "id": "2zHvKSpo7nX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pollution_data['pm10'].max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-Ikgdphzdha",
        "outputId": "80eccfdd-5a77-48a7-83db-d009bcabf069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87341.71"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "what is the maximum pm10 value"
      ],
      "metadata": {
        "id": "RgKB2UJo7rWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pollution_data['pm10'].min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqANkwbXzmF5",
        "outputId": "87a1032c-eaf5-45a1-a46c-535c0eaaff49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joined_table = Table().from_df(pollution_data)"
      ],
      "metadata": {
        "id": "zsjLIScVmM9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "show the first few rows of the table?"
      ],
      "metadata": {
        "id": "KsUXysj_B1T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_table.show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "lhgoak8vzEfS",
        "outputId": "9073dd57-f354-484e-a375-be257b93b53b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>pm10</th> <th>neighborhood</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>11.35</td> <td>Bronx Park  </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1.18 </td> <td>Bronx Park  </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>\n",
              "<p>... (118493 rows omitted)</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "print minimum and maximum pm10 values?"
      ],
      "metadata": {
        "id": "zgAZOxbYB56N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pm10 = joined_table.column('pm10')\n",
        "min(pm10), max(pm10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19ncrqo0y8l5",
        "outputId": "e02c5cfe-c3ca-4579-ea89-a334dc7959e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 87341.710000000006)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instructions for Students\n",
        "-You task is to analyze NYC hyperlocal air quality data using the provided dataset. Complete the following tasks in your Jupyter Notebook.total of 5 marks . Use the Table abstraction."
      ],
      "metadata": {
        "id": "kkbu-cal0QRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks"
      ],
      "metadata": {
        "id": "lNeAe-ZH0eL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = blue size  = 5>**Task 1: Sampling and Empirical Distribution (1 Mark)**\n",
        "\n",
        "**Task Description:**\n",
        "\n",
        "- Randomly sample 10% of the rows from the joined_table without replacement.\n",
        "- Compute the mean pm10 value for this sample.\n",
        "- Create an empirical histogram of the pm10 values from the sampled data using 10 bins.\n",
        "- Comment on the shape of the histogram and compare it to the distribution of the full dataset."
      ],
      "metadata": {
        "id": "lTICc0fUdXNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color = red size= 5> attention</font>\n",
        "\n",
        "remove pm10 values that are unreasonably high (above 300 µg/m³)"
      ],
      "metadata": {
        "id": "H9u190hzeLnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_table = joined_table.where('pm10', are.below(300))"
      ],
      "metadata": {
        "id": "iXqSs3EDd7v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(joined_table)"
      ],
      "metadata": {
        "id": "XBQxvwlie-pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_table.num_rows"
      ],
      "metadata": {
        "id": "kLHMmWdHe6rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly sample 10% of the rows from the joined_table without replacement.\n",
        "sampled_table = joined_table.sample(k=int(joined_table.num_rows * 0.1), with_replacement=False)\n",
        "\n",
        "# Display the sampled table\n",
        "sampled_table.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "JiEGMMoof-gP",
        "outputId": "7b538871-ff4d-42fd-e74b-396f024b4394"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'joined_table' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0c015fdf8c3d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Randomly sample 10% of the rows from the joined_table without replacement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msampled_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoined_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoined_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_replacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display the sampled table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msampled_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'joined_table' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the mean pm10 value for this sample.\n",
        "mean_pm10_sampled = np.mean(sampled_table.column('pm10'))\n",
        "\n",
        "# Print the mean value\n",
        "print(f\"Mean pm10 value in the sample: {mean_pm10_sampled}\")"
      ],
      "metadata": {
        "id": "IT94vQ9QgDmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empirical histogram of the pm10 values from the sampled data using 10 bins.\n",
        "sampled_table.hist('pm10', bins=10)\n",
        "plt.title('Empirical Histogram of PM10 Values (Sampled Data)')\n",
        "plt.xlabel('PM10 Value (µg/m³)')\n",
        "plt.ylabel('Frequency');"
      ],
      "metadata": {
        "id": "MlKcXXokgHcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The histogram of the 10% sampled pm10 data shows a right-skewed distribution, with most values concentrated at the lower end below 100 µg/m³ and a gradual tapering toward higher values. This pattern is consistent with the full dataset after removing outliers, which also exhibits a right-skewed shape.\n",
        "# The similarity in distribution between the sample and the full dataset suggests that the sample is representative of the population and captures the underlying structure of the data well."
      ],
      "metadata": {
        "id": "0Pe7mO60gKrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = blue size  = 5> **Task 2: Estimation and Confidence Intervals (1 Mark)**\n"
      ],
      "metadata": {
        "id": "CuJ4xIHXeZXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task Description:**\n",
        "- Use the bootstrap method to estimate the median pm10 value for the entire dataset.\n",
        "- Generate 5,000 bootstrap samples and compute the median for each sample.\n",
        "- Construct a 95% confidence interval for the population median using the 2.5th and 97.5th percentiles of the bootstrapped medians.\n",
        "- Visualize the results by drawing an `empirical histogram` of the bootstrapped medians and overlaying the confidence interval on the horizontal axis.\n",
        "- Report the confidence interval and interpret what it means in the context of air quality."
      ],
      "metadata": {
        "id": "w3fxSNsIq7RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the bootstrap method to estimate the median pm10 value for the entire dataset.\n",
        "def bootstrap_median(original_data, num_repetitions):\n",
        "  medians = []\n",
        "  for _ in range(num_repetitions):\n",
        "    # Resample the data with replacement\n",
        "    bootstrap_sample = np.random.choice(original_data, size=len(original_data), replace=True)\n",
        "    # Calculate the median of the bootstrap sample\n",
        "    median = np.median(bootstrap_sample)\n",
        "    medians.append(median)\n",
        "  return np.array(medians)\n",
        "\n",
        "# Get the pm10 values from the joined_table\n",
        "pm10_values = joined_table.column('pm10')\n",
        "\n",
        "# Generate 5,000 bootstrap samples and compute the median for each sample\n",
        "bootstrapped_medians = bootstrap_median(pm10_values, num_repetitions=5000)\n",
        "\n",
        "# Print the estimated median\n",
        "estimated_median = np.median(bootstrapped_medians)\n",
        "print(f\"Estimated median pm10 value: {estimated_median}\")\n",
        "\n",
        "# Construct a 95% confidence interval for the population median using the 2.5th and 97.5th percentiles of the bootstrapped medians.\n",
        "confidence_interval = np.percentile(bootstrapped_medians, [2.5, 97.5])\n",
        "\n",
        "# Visualize the results by drawing an empirical histogram of the bootstrapped medians and overlaying the confidence interval on the horizontal axis.\n",
        "plt.hist(bootstrapped_medians, bins=30, edgecolor='black')\n",
        "plt.title('Bootstrap Distribution of Medians (PM10)')\n",
        "plt.xlabel('PM10 Median (µg/m³)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Overlay the confidence interval\n",
        "plt.axvline(confidence_interval[0], color='yellow', linestyle='--', label='2.5th percentile')\n",
        "plt.axvline(confidence_interval[1], color='yellow', linestyle='--', label='97.5th percentile')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Report the confidence interval\n",
        "print(f\"95% Confidence Interval for Population Median PM10: {confidence_interval}\")\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"We are 95% confident that the true median PM10 level in the population lies between\",\n",
        "      f\"{confidence_interval[0]:.2f} µg/m³ and {confidence_interval[1]:.2f} µg/m³.\")\n",
        "print(\"This means that if we were to repeatedly sample from the population and calculate the median PM10 level,\",\n",
        "      \"95% of the time the calculated median would fall within this interval.\")\n",
        "print(\"In the context of air quality, this interval provides an estimate of the typical PM10 concentration\",\n",
        "      \"and helps us understand the range of values we can expect to see in the population.\")"
      ],
      "metadata": {
        "id": "qY_54E6lgVPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = blue size  = 5> **Task 3: Hypothesis Testing Using Confidence Intervals (1.5 Mark)**"
      ],
      "metadata": {
        "id": "9cs7K_Q3icPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task Description:**\n",
        "- Test the hypothesis about the average <font color = red size =5> `pm10` </font> level in the population using confidence intervals.\n",
        "  - Null Hypothesis (H0): The average pm10 level in the population is `20 μg/m³` .\n",
        "  - Alternative Hypothesis (Ha): The average pm10 level in the population is not `20 μg/m³` .\n",
        "- Use the confidence interval method to test this hypothesis:\n",
        "Construct a 95% confidence interval for the average `pm10` level in the population.\n",
        "  - If the confidence interval contains `20 μg/m³` , conclude that there is no significant difference from `20 μg/m³` (fail to reject H0).\n",
        "  - Otherwise, conclude that there is a significant difference (reject H0).\n",
        "- Interpret your results and explain whether the data supports the null hypothesis or the alternative hypothesis."
      ],
      "metadata": {
        "id": "Ukmexr5YivJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the pm10 values from the joined_table\n",
        "pm10_values = joined_table.column('pm10')\n",
        "\n",
        "# Calculate the sample mean and standard deviation\n",
        "sample_mean = np.mean(pm10_values)\n",
        "sample_std = np.std(pm10_values)\n",
        "\n",
        "# Calculate the standard error of the mean\n",
        "std_err = sample_std / np.sqrt(len(pm10_values))\n",
        "\n",
        "# Calculate the margin of error for a 95% confidence interval\n",
        "margin_of_error = 1.96 * std_err  # 1.96 is the z-score for a 95% CI\n",
        "\n",
        "# Calculate the confidence interval\n",
        "confidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n",
        "\n",
        "# Print the confidence interval\n",
        "print(f\"95% Confidence Interval for Average PM10: {confidence_interval}\")\n",
        "\n",
        "# Hypothesis testing\n",
        "null_hypothesis_value = 20\n",
        "\n",
        "# Check if the null hypothesis value is within the confidence interval\n",
        "if null_hypothesis_value >= confidence_interval[0] and null_hypothesis_value <= confidence_interval[1]:\n",
        "    print(\"\\nConclusion: Fail to reject the null hypothesis.\")\n",
        "    print(\"The data does not provide enough evidence to conclude that the average pm10 level is different from 20 μg/m³.\")\n",
        "else:\n",
        "    print(\"\\nConclusion: Reject the null hypothesis.\")\n",
        "    print(\"The data provides strong evidence to suggest that the average pm10 level is significantly different from 20 μg/m³.\")\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"The 95% confidence interval for the average pm10 level in the population is\",\n",
        "      f\"({confidence_interval[0]:.2f}, {confidence_interval[1]:.2f}) μg/m³.\")\n",
        "print(\"Since the null hypothesis value of 20 μg/m³\",\n",
        "      \"falls within/outside\" if null_hypothesis_value >= confidence_interval[0] and null_hypothesis_value <= confidence_interval[1] else \"falls outside\",\n",
        "      \"this interval, we\",\n",
        "      \"fail to reject/reject\" if null_hypothesis_value >= confidence_interval[0] and null_hypothesis_value <= confidence_interval[1] else \"reject\",\n",
        "      \"the null hypothesis.\")\n",
        "print(\"This means that there\",\n",
        "      \"is not/is\" if null_hypothesis_value >= confidence_interval[0] and null_hypothesis_value <= confidence_interval[1] else \"is\",\n",
        "      \"sufficient evidence to suggest that the true average pm10 level in the population is different from 20 μg/m³.\")"
      ],
      "metadata": {
        "id": "6dyZ4IRsgf2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = blue size  = 5> **Task 4: Percentiles and Extreme Values (1.5 Mark)**\n"
      ],
      "metadata": {
        "id": "A7cJxidKmB31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Calculate mean pm10 for each neighborhood\n",
        "neighborhood_means = joined_table.group('neighborhood', np.mean).sort('pm10 mean', descending=True)\n",
        "\n",
        "# 2. Get top 3 neighborhoods\n",
        "top_3_neighborhoods = neighborhood_means.take(np.arange(3)).column('neighborhood')\n",
        "\n",
        "# 3. Bootstrap function\n",
        "def bootstrap_mean(data, num_repetitions):\n",
        "  means = []\n",
        "  for _ in range(num_repetitions):\n",
        "    bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
        "    mean = np.mean(bootstrap_sample)\n",
        "    means.append(mean)\n",
        "  return np.array(means)\n",
        "\n",
        "# 4. Loop through top 3 neighborhoods, bootstrap, and plot\n",
        "for neighborhood in top_3_neighborhoods:\n",
        "  # Filter data for the current neighborhood\n",
        "  neighborhood_data = joined_table.where('neighborhood', are.equal_to(neighborhood)).column('pm10')\n",
        "\n",
        "  # Generate bootstrap samples\n",
        "  bootstrapped_means = bootstrap_mean(neighborhood_data, num_repetitions=5000)\n",
        "\n",
        "  # Calculate confidence interval\n",
        "  confidence_interval = np.percentile(bootstrapped_means, [2.5, 97.5])\n",
        "\n",
        "  # Create histogram\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.hist(bootstrapped_means, bins=30, edgecolor='black')\n",
        "  plt.title(f'Bootstrap Distribution of Mean PM10 for {neighborhood}')\n",
        "  plt.xlabel('Mean PM10 (µg/m³)')\n",
        "  plt.ylabel('Frequency')\n",
        "\n",
        "  # Overlay confidence interval\n",
        "  plt.axhline(confidence_interval[0], color='yellow', linestyle='--', label='2.5th percentile')\n",
        "  plt.axhline(confidence_interval[1], color='yellow', linestyle='--', label='97.5th percentile')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  print(f\"95% Confidence Interval for {neighborhood}: {confidence_interval}\")"
      ],
      "metadata": {
        "id": "V3s-bHwCghK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Use the bootstrap method to simulate the mean `pm10` levels for each neighborhood. Generate 5,000 bootstrap samples for the top 3 neighborhoods with the highest mean pm10 levels.\n",
        "- Create an empirical histogram for each of these neighborhoods, showing the distribution of the bootstrapped means.\n",
        "- Overlay horizontal yellow lines to indicate the 95% confidence interval for the mean pm10 level in each neighborhood.\n"
      ],
      "metadata": {
        "id": "x1hoJVDEoIOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission Guidelines\n",
        "- Add a \"Open in Colab\" button at the top of your notebook using the following Markdown code:\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo-path/notebook.ipynb)\n",
        "\n",
        "- Upload your completed Jupyter Notebook to a GitHub repository.\n",
        "- Submit the link to your GitHub repository in the Blackboard LMS along with the Jupyter solution file.\n",
        "- <font color = red size = 6> ATTENTION!!! </font> Students are encouraged to work on groups, however the submission should be individual and each student should have her/his own unique final assignment solution, which is to be submitted in BB"
      ],
      "metadata": {
        "id": "kM16Gy602H1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grading Rubric\n",
        "based on the following criteria:\n",
        "\n",
        "- Correctness : The solution produces the expected output using the Table abstraction .\n",
        "- Clarity : Code is well-organized, readable, and includes comments explaining key steps.\n",
        "- Creativity : Visualizations and analyses are presented in an engaging and insightful manner."
      ],
      "metadata": {
        "id": "dY2-OB4A2huX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hints for Success\n",
        "- Use the Table abstraction methods like .select(), .where(), .group(), .apply(), and .sample() for data manipulation.\n",
        "- Refer to the slides and examples from book Chapters 10, 11, and 13, for guidance on Sampling and Empirical Distributions, Testing Hypotheses,  and Estimation.\n",
        "- Test your code frequently to ensure it runs without errors."
      ],
      "metadata": {
        "id": "0GlfWQnV2r9v"
      }
    }
  ]
}